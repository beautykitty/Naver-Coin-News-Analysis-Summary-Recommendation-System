{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ÎÑ§Ïù¥Î≤Ñ Í∏∞ÏÇ¨ ÌÅ¨Î°§ÎßÅ.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m5nMdzz4-rN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "import time\n",
        "import datetime\n"
      ],
      "metadata": {
        "id": "XDDW8iAN5MKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}\n",
        "\n",
        "\n",
        "def NAVER_news_scraper(maxpage, query, s_date, e_date):\n",
        "    \n",
        "    s_from = s_date.replace(\".\", \"\")\n",
        "    e_to = e_date.replace(\".\", \"\")\n",
        "    \n",
        "    \n",
        "    news_df = pd.DataFrame(columns=(\"Title\",\"Datetime\", \"Article\",'Newscompany','Link'))\n",
        "    index = 0 \n",
        "        \n",
        "    page = 1\n",
        "    maxpage_t =(int(maxpage)-1)*10+1  \n",
        "\n",
        "    \n",
        "    while page <= maxpage_t:\n",
        "        # \\ Îí§Ïóê Ïñ¥Îñ§ ÎπàÏπ∏ÎèÑ Îì§Ïñ¥Ïò§Î©¥ ÏïàÎêúÎã§. \n",
        "        url = 'https://search.naver.com/search.naver?where=news&query=' + \\\n",
        "            query + '&ds=' + s_date + '&de=' + e_date + '&nso=so:r,p:from' + \\\n",
        "            s_from + 'to' + e_to + ',a:all&start=' + str(page)\n",
        "        \n",
        "\n",
        "        print('‚ñ∂ ÌéòÏù¥ÏßÄ Î≤àÌò∏ = ', page)\n",
        "        req = requests.get(url)\n",
        "        \n",
        "        cont = req.content\n",
        "        soup = BeautifulSoup(cont, 'html.parser')\n",
        "## idÎäî Í∑∏ÌéòÏù¥ÏßÄÏóêÏÑú Í≥†Ïú†Ìïú Í∞í         \n",
        "        for urls in soup.find_all('a','info'): \n",
        "                \n",
        "                try:\n",
        "                    \n",
        "                    if urls['href'].startswith(\"https://n.news.naver.com/\") :      \n",
        "\n",
        "                        newsreq = requests.get(urls[\"href\"], headers = headers) # header Î∞òÎìúÏãú ÎÑ£Ïñ¥ÏïºÌï®\n",
        "                            \n",
        "                        bsoup = BeautifulSoup(newsreq.content, 'html.parser')\n",
        "                            \n",
        "                        title = bsoup.select('h2.media_end_head_headline')[0].text\n",
        "                        \n",
        "                        newsdate = bsoup.select('div.media_end_head_info_datestamp')[0].text[6:14]\n",
        "        \n",
        "                        btext = bsoup.find(id='dic_area').text.replace('\\n', \" \").replace('\\t', \" \")    \n",
        "                        article = btext.replace(\"// flash Ïò§Î•òÎ•º Ïö∞ÌöåÌïòÍ∏∞ ÏúÑÌïú Ìï®Ïàò Ï∂îÍ∞Ä function _flash_removeCallback() {}\", \"\")\n",
        "                        \n",
        "                        newscompany = bsoup.select('img')[0]['alt']    \n",
        "                              \n",
        "                        url = urls[\"href\"]\n",
        "                            \n",
        "                        news_df.loc[index] = [title,newsdate,article,newscompany, url]             \n",
        "                        index += 1                         \n",
        "\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "            \n",
        "        page += 10\n",
        "            \n",
        "    print()\n",
        "    print('Completed! üåûüåûüåûüåûüåûüåûüåûüåûüåûüåûüåûüåûüåûüåûüåûüåûüåûüåûüåûüåûüåû ')\n",
        "    \n",
        "    return news_df \n",
        "    \n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "WDL2XDGk5MMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ÎπÑÌä∏ÏΩîÏù∏\n",
        "# 100\n",
        "# 2022.05.09\n",
        "# 2022.06.09\n",
        "\n",
        "query = input(\"‚óé Í≤ÄÏÉâÏñ¥: \")\n",
        "\n",
        "maxpage = input(\"‚óé Í≤ÄÏÉâ Ìï† ÌéòÏù¥ÏßÄÏàò: \")    # Ï¢ÖÎ£å ÎÇ†ÏßúÎ∂ÄÌÑ∞ ÏãúÏûëÌïòÏó¨ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú ÏãúÏûë - Í≤ÄÏÉâ ÌéòÏù¥ÏßÄ ÏàòÏóê ÏùòÌï¥ Ï†úÌïúÎê† Ïàò ÏûàÏùå  \n",
        "\n",
        "s_date = input(\"‚óé ÏãúÏûë ÎÇ†Ïßú(YYYY.MM.DD): \")\n",
        "\n",
        "e_date = input(\"‚óé Ï¢ÖÎ£å ÎÇ†Ïßú(YYYY.MM.DD): \")\n",
        "\n",
        "\n",
        "start = time.time()    # ÏãúÏûë ÏãúÍ∞Ñ Í∏∞Î°ù \n",
        "    \n",
        "\n",
        "news_df = NAVER_news_scraper(maxpage, query, s_date, e_date)\n",
        "\n",
        "\n",
        "processing_time = time.time() - start    # Í≤ΩÍ≥º ÏãúÍ∞Ñ Í≥ÑÏÇ∞  \n",
        "times = str(datetime.timedelta(seconds = processing_time))   \n",
        "\n",
        "print()\n",
        "print(\"‚óé ÏàòÌñâÏãúÍ∞Ñ = \", times)\n",
        "\n"
      ],
      "metadata": {
        "id": "U04bsG-A5MOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df"
      ],
      "metadata": {
        "id": "Znovt5-WeJTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ÏàòÏßëÌïú Îç∞Ïù¥ÌÑ∞Î•º ÌôîÏùºÎ°ú Ï†ÄÏû• \n",
        "\n",
        "s_from = s_date.replace(\".\", \"\")\n",
        "e_to = e_date.replace(\".\", \"\")\n",
        "\n",
        "file_path =  \"/content/gdrive/My Drive/ÏõπÍ≥ºÌÖçÏä§Ìä∏ÎßàÏù¥Îãù/Í∏∞Îßê ÌîÑÎ°úÏ†ùÌä∏/Data/\"\n",
        "\n",
        "file_name = \"ÎÑ§Ïù¥Î≤Ñ_Îâ¥Ïä§_\" + query + \"_\" + s_from + \"_\" + e_to + \".csv\"\n",
        "\n",
        "file_full_path = file_path + file_name\n",
        "\n",
        "news_df.to_csv(file_full_path ,  index = False, encoding=\"utf-8-sig\") # utf-8-sigÎ°ú Ìï¥ÏïºÌï®!  \n"
      ],
      "metadata": {
        "id": "LBwEt4vo5MQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ÏΩîÏù∏\n",
        "# 100\n",
        "# 2022.05.09\n",
        "# 2022.06.09\n",
        "\n",
        "query = input(\"‚óé Í≤ÄÏÉâÏñ¥: \")\n",
        "\n",
        "maxpage = input(\"‚óé Í≤ÄÏÉâ Ìï† ÌéòÏù¥ÏßÄÏàò: \")    # Ï¢ÖÎ£å ÎÇ†ÏßúÎ∂ÄÌÑ∞ ÏãúÏûëÌïòÏó¨ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú ÏãúÏûë - Í≤ÄÏÉâ ÌéòÏù¥ÏßÄ ÏàòÏóê ÏùòÌï¥ Ï†úÌïúÎê† Ïàò ÏûàÏùå  \n",
        "\n",
        "s_date = input(\"‚óé ÏãúÏûë ÎÇ†Ïßú(YYYY.MM.DD): \")\n",
        "\n",
        "e_date = input(\"‚óé Ï¢ÖÎ£å ÎÇ†Ïßú(YYYY.MM.DD): \")\n",
        "\n",
        "\n",
        "start = time.time()    # ÏãúÏûë ÏãúÍ∞Ñ Í∏∞Î°ù \n",
        "    \n",
        "\n",
        "news_df = NAVER_news_scraper(maxpage, query, s_date, e_date)\n",
        "\n",
        "\n",
        "processing_time = time.time() - start    # Í≤ΩÍ≥º ÏãúÍ∞Ñ Í≥ÑÏÇ∞  \n",
        "times = str(datetime.timedelta(seconds = processing_time))   \n",
        "\n",
        "print()\n",
        "print(\"‚óé ÏàòÌñâÏãúÍ∞Ñ = \", times)\n",
        "\n"
      ],
      "metadata": {
        "id": "CIr2qL8xJ1El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ÏàòÏßëÌïú Îç∞Ïù¥ÌÑ∞Î•º ÌôîÏùºÎ°ú Ï†ÄÏû• \n",
        "\n",
        "s_from = s_date.replace(\".\", \"\")\n",
        "e_to = e_date.replace(\".\", \"\")\n",
        "\n",
        "file_path =  \"/content/gdrive/My Drive/ÏõπÍ≥ºÌÖçÏä§Ìä∏ÎßàÏù¥Îãù/Í∏∞Îßê ÌîÑÎ°úÏ†ùÌä∏/Data/\"\n",
        "\n",
        "file_name = \"ÎÑ§Ïù¥Î≤Ñ_Îâ¥Ïä§_\" + query + \"_\" + s_from + \"_\" + e_to + \".csv\"\n",
        "\n",
        "file_full_path = file_path + file_name\n",
        "\n",
        "news_df.to_csv(file_full_path ,  index = False, encoding=\"utf-8-sig\") # utf-8-sigÎ°ú Ìï¥ÏïºÌï®!  "
      ],
      "metadata": {
        "id": "hNGmJhID5MSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df.shape"
      ],
      "metadata": {
        "id": "7IZOOW1A5MUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xgipa_m55MV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_FOkV_IW5MX1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}